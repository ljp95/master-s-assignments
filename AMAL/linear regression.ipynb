{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ne pas oublier d'executer dans le shell avant de lancer python :\n",
    "# source /users/Enseignants/piwowarski/venv/amal/3.7/bin/activate\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "from datamaestro import prepare_dataset \n",
    "import numpy as np\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Very simplified context object\"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "class Linear(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,x,w,b):\n",
    "        ctx.save_for_backward(x,w,b)\n",
    "        return torch.mm(x,torch.t(w))+b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        x,w,b = ctx.saved_tensors\n",
    "        dL_x = grad_output*w\n",
    "        dL_w = grad_output*x\n",
    "        dL_b = grad_output\n",
    "        return dL_x,dL_w,dL_b\n",
    "    \n",
    "class MSE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx_mse,y,yhat):\n",
    "        ctx_mse.save_for_backward(y,yhat)\n",
    "        tmp = yhat-y\n",
    "        return torch.mul(tmp,tmp)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx_mse):\n",
    "        y,yhat = ctx_mse.saved_tensors\n",
    "        return 2*(yhat-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERIF BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.9181, -1.0957,  0.4695, -1.0051,  1.3708]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>), tensor([[-0.4293, -0.3487, -0.3684, -0.3479,  0.2376],\n",
      "        [-0.4214, -0.8073,  0.8854, -0.5931,  0.6962],\n",
      "        [-1.3283,  2.3737,  1.5293,  0.2929,  1.5791],\n",
      "        [-1.1369,  0.7733, -0.1657, -0.7762, -0.9779],\n",
      "        [ 0.4702,  0.9757,  1.1605,  1.1399, -1.0780],\n",
      "        [ 0.6794,  0.4269, -0.5760, -1.2367,  0.3724],\n",
      "        [ 0.7432, -0.1651, -0.4219, -0.6452,  0.6890],\n",
      "        [-0.1045,  0.9341, -0.7687, -0.1168,  2.2778],\n",
      "        [ 0.5454, -0.0345,  0.7703,  1.8453, -0.0635],\n",
      "        [ 0.4412,  1.4555,  0.7170, -1.5096, -0.3525]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>), 1)\n",
      "Check backpropagation :  True\n"
     ]
    }
   ],
   "source": [
    "# Pour utiliser la fonction \n",
    "linear1 = Linear()\n",
    "ctx1 = Context()\n",
    "x = torch.randn(10,5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(1,5,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(1,1,requires_grad=True,dtype=torch.float64)\n",
    "output = linear1.forward(ctx1,x,w,b)\n",
    "\n",
    "grad_output = linear1.backward(ctx1,1)\n",
    "print(grad_output)\n",
    "## Pour tester le gradient \n",
    "check = linear1.apply\n",
    "print(\"Check backpropagation : \",torch.autograd.gradcheck(check,(x,w,b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour telecharger le dataset Boston\n",
    "ds=prepare_dataset(\"edu.uci.boston\")\n",
    "fields, data =ds.files.data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = X.mean((0))\n",
    "    std = X.std((0))\n",
    "    return (X-mean)/std,mean,std\n",
    "\n",
    "def loss(X,Y,w,b):\n",
    "    return torch.mean((torch.mm(X,torch.t(w))+b-Y)**2)\n",
    "\n",
    "def simple_split(X,Y,p):\n",
    "    end = int(p*X.shape[0])\n",
    "    X_train,Y_train = X[0:end,:],Y[0:end,:]\n",
    "    X_test,Y_test = X[end:,:],Y[end:,:]\n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOCHASTIC GRADIENT DESCENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(layer,cost,X_train,Y_train,X_test,Y_test,nb_desc,eta,w,b):\n",
    "    train_cost,test_cost = [],[]\n",
    "    N = X_train.shape[0]\n",
    "    if w is None:\n",
    "        w = torch.randn(c,d,requires_grad=True,dtype=torch.float64)\n",
    "    if b is None:\n",
    "        b = torch.randn(1,1,requires_grad=True,dtype=torch.float64)\n",
    "    for i in range(nb_desc):\n",
    "        indice = torch.randint(0,N,size=(1,1))\n",
    "        x,y = X_train[indice][0],Y_train[indice][0]\n",
    "        #forward\n",
    "        yhat = layer.forward(ctx1,x,w,b)\n",
    "        mse = cost.forward(ctx_mse,y,yhat)\n",
    "        #keep cost\n",
    "        train_cost.append(loss(X_train,Y_train,w,b))\n",
    "        test_cost.append(loss(X_test,Y_test,w,b))\n",
    "        #backpropagation\n",
    "        grad_output = cost.backward(ctx_mse)\n",
    "        dl_x,dl_w,dl_b = layer.backward(ctx1,grad_output)\n",
    "        #update\n",
    "        w = w - eta*dl_w\n",
    "        b = b - eta*dl_b\n",
    "    return train_cost,test_cost,w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class\n",
    "layer = Linear()\n",
    "ctx1 = Context()\n",
    "mse = MSE()\n",
    "ctx_mse = Context()\n",
    "\n",
    "#hyperparameters\n",
    "p = 0.7 #for split\n",
    "nb_desc = 15000\n",
    "eta = 0.00001\n",
    "\n",
    "#prepare data\n",
    "X,Y = data[:,0:-1],data[:,-1]\n",
    "X,Y = torch.from_numpy(X),torch.from_numpy(Y.reshape((Y.shape[0],1)))\n",
    "X,meanX,stdX = normalize(X)\n",
    "Y,meanY,stdY = normalize(Y)\n",
    "d,c = X.shape[1],Y.shape[1]\n",
    "X_train,Y_train,X_test,Y_test = simple_split(X,Y,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reinitialize parameters\n",
    "w = torch.randn(c,d,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(c,1,requires_grad=True,dtype=torch.float64)\n",
    "train_cost,test_cost = [],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cost : 8.217651050338526\n",
      "Test cost : 11.932371706039605\n"
     ]
    }
   ],
   "source": [
    "(tmp_train,tmp_test,w,b) = gradient_descent(layer,mse,X_train,Y_train,X_test,Y_test,nb_desc,eta,w,b)\n",
    "train_cost += tmp_train\n",
    "test_cost += tmp_test\n",
    "print(\"Train cost : {}\\nTest cost : {}\".format(train_cost[-1],test_cost[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN/TEST CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "for i in range(len(train_cost)):\n",
    "    writer.add_scalar('Train_cost', train_cost[i].item(), i)\n",
    "    writer.add_scalar('Test_cost', test_cost[i].item(), i)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
