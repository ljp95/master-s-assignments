{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlecolab = False\n",
    "\n",
    "if googlecolab:\n",
    "    from os.path import exists\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "    !pip install Pillow==4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperpameters\n",
    "\n",
    "Define the hyperparameters. You can play with those later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loading\n",
    "\n",
    "workers = 4 # Number of workers for dataloader (/!\\ set to 4 when you're done debugging)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "arch = 'cDCGAN' # or cGAN\n",
    "nz = 100 # Size of z latent vector (i.e. size of generator input)Âµ\n",
    "ndf = 32 # Base size of feature maps in discriminator\n",
    "ngf = 32 # Base size of feature maps in generator\n",
    "\n",
    "## Optimization\n",
    "\n",
    "lrD = 0.0002 # Learning rate for the discriminator\n",
    "lrG = 0.0002 # Learning rate for the generator\n",
    "beta1G = 0.5 # Momentum beta1 for the discriminator\n",
    "beta1D = 0.5 # Momentum beta1 for the generator\n",
    "\n",
    "## Training\n",
    "\n",
    "batch_size = 128 # Images per batch\n",
    "nb_update_D = 1 # Number of sub-steps of discriminator optim. at each step\n",
    "nb_update_G = 1 # Number of sub-steps of generator optim. at each step\n",
    "#steps = 8000 # Number of global steps in the training loop\n",
    "nb_epochs = 20 # Number of epochs, leave \"None\" if you want to set the number of \"steps\" (i.e. batches)\n",
    "\n",
    "\n",
    "if nb_epochs is None:\n",
    "    nb_epochs = (steps * batch_size) / (nb_update_D * 50000)\n",
    "else:\n",
    "    steps = int(nb_epochs * nb_update_D * 50000 / batch_size)\n",
    "print(\"Doing %.1f epochs in %d steps\" % (nb_epochs, steps))\n",
    "steps_per_epoch = int(steps / nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Download and load the dataset. Nothing to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataroot = '/tmp/mnist'\n",
    "transform = transforms.Compose([\n",
    "        transforms.Pad(2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "dataset = dset.MNIST(dataroot, train=True, download=True, transform=transform)\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architectures\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "Input: Image $x \\in \\mathbb{R}^{32\\times 32\\times 1}$  \n",
    "Output: \"Real\" image probability $\\in [0,1]$\n",
    "\n",
    "## Generator\n",
    "\n",
    "Input: Random \"noise\" $z \\in \\mathbb{R}^{\\text{nz}}$  \n",
    "Output: Generated image $\\tilde x \\in \\mathbb{R}^{32\\times 32\\times 1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cDCGAN\n",
    "\n",
    "if arch == 'cDCGAN':\n",
    "    \n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.emb_x = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Conv2d(in_channels=1, out_channels=2*ndf, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "            )\n",
    "            self.emb_y = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=10, out_channels=2*ndf, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "            )\n",
    "            self.emb_xy = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Conv2d(in_channels=4*ndf, out_channels=8*ndf,kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(8*ndf),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "\n",
    "                nn.Conv2d(8*ndf,16*ndf,4,2,1,bias=False),\n",
    "                nn.BatchNorm2d(16*ndf),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "\n",
    "                nn.Conv2d(16*ndf,1,4,1,0,bias=True),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            #TODO\n",
    "            x = self.emb_x(x)\n",
    "            y = torch.ones((10,32,32)).to(device)*y\n",
    "            y = self.emb_y(y)\n",
    "            o = self.emb_xy(torch.cat((x,y),dim=1))\n",
    "            return o\n",
    "\n",
    "\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.emb_z = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.ConvTranspose2d(in_channels=nz, out_channels=8*ndf,kernel_size=4, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(8*ndf),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.emb_y = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.ConvTranspose2d(in_channels=10, out_channels=8*ndf,kernel_size=4, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(8*ndf),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.emb_zy = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.ConvTranspose2d(16*ndf,8*ndf,4,2,1,bias=False),\n",
    "                nn.BatchNorm2d(8*ndf),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.ConvTranspose2d(8*ndf,4*ndf,4,2,1,bias=False),\n",
    "                nn.BatchNorm2d(4*ndf),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.ConvTranspose2d(4*ndf,1,4,2,1,bias=True),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, z, y):\n",
    "            # TODO\n",
    "            z = self.emb_z(z)\n",
    "            y = self.emb_y(y)\n",
    "            o = self.emb_zy(torch.cat((z,y),dim=1))\n",
    "            return o\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cGAN\n",
    "\n",
    "if arch == 'cGAN':\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.emb_x = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Linear(1024,1024, bias = True),\n",
    "                nn.LeakyReLU(negative_slope = 0.2)\n",
    "            )\n",
    "            self.emb_y = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Linear(10,1024, bias = True),\n",
    "                nn.LeakyReLU(negative_slope = 0.2)\n",
    "            )\n",
    "            self.emb_xy = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Linear(2048,1024, bias=False),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "                \n",
    "                nn.Linear(1024,512, bias=False),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "                \n",
    "                nn.Linear(512,1, bias=True),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, y):\n",
    "            # Remove unused dimensions in non-conv model\n",
    "            x = x.view(x.shape[0], -1) #z[:, :, 0, 0]\n",
    "            y = y.view(y.shape[0], -1)          \n",
    "            #TODO\n",
    "            x = self.emb_x(x)\n",
    "            y = self.emb_y(y)\n",
    "            o = self.emb_xy(torch.cat((x,y),dim=1))\n",
    "            return o\n",
    "\n",
    "\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Generator, self).__init__()\n",
    "            self.emb_z = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Linear(nz,256, bias=False),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "            )\n",
    "            self.emb_y = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Linear(10,256, bias=False),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.LeakyReLU(negative_slope = 0.2),\n",
    "            )\n",
    "            self.emb_zy = nn.Sequential(\n",
    "                # TODO\n",
    "                nn.Linear(512,512, bias=False),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.Linear(512,1024, bias=False),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                nn.Linear(1024,1024, bias=True),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, z, y):\n",
    "            # Remove unused dimensions in non-conv model\n",
    "            z = z.view(z.shape[0], -1) #z[:, :, 0, 0]\n",
    "            y = y.view(y.shape[0], -1)\n",
    "            \n",
    "            # TODO\n",
    "            z = self.emb_z(z)\n",
    "            y = self.emb_y(y)\n",
    "            o = self.emb_zy(torch.cat((z,y),dim=1))\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the models\n",
    "print(netG)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test you models to check if they \n",
    "z = torch.zeros(10, nz, 1, 1).to(device)\n",
    "x = torch.zeros(10, 1, 32, 32).to(device)\n",
    "y = torch.randn(10, 10, 1, 1).to(device)\n",
    "print(netG(z, y).shape) # expected: [10, 1, 32, 32]\n",
    "print(netD(x, y).shape) # expected: [10, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and training\n",
    "\n",
    "Here we will define:\n",
    "* Our prior $P(z)$ that we use to sample random \"noise\". We will use a Gaussian distribution.\n",
    "* The criterion that will be used to train the discriminator, and indirectly the generator. We will use the binary cross-entropy.\n",
    "* The optimizers of both models. We will use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior P(z). Returns a Gaussian random tensor of shape (batch_size, nz, 1, 1)\n",
    "def get_noise(batch_size):\n",
    "    noise = torch.randn((batch_size,nz,1,1)).to(device) # TODO\n",
    "    return noise\n",
    "\n",
    "# Returns a random vector of one-hot encoding of shape (batch_size, 10, 1, 1)\n",
    "def get_fixed_y(batch_size):\n",
    "    y = torch.zeros(batch_size, 10, 1, 1, device=device)\n",
    "    n = batch_size // 10\n",
    "    for i in range(10):\n",
    "        y[i*n:i*n+n, i, :, :].fill_(1)\n",
    "    return y\n",
    "\n",
    "# Create the criterion function that will take (y_hat, y) as input\n",
    "criterion = nn.BCELoss() # TODO\n",
    "\n",
    "# Setup Adam optimizers for D and G\n",
    "optimizerD = optim.Adam(netD.parameters(),lr=lrD, betas=(beta1D,0.999)) # TODO\n",
    "optimizerG = optim.Adam(netG.parameters(),lr=lrG, betas=(beta1G,0.999))  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format / batch creation functions\n",
    "\n",
    "`r_real` and `r_fake` are targets for the disriminator's criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data format / batch creation functions\n",
    "\n",
    "fixed_noise = get_noise(100) # Create a fixed random vector sampled from a Gaussian, will be used during train for viz\n",
    "fixed_y = get_fixed_y(100)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "iterator = iter(dataloader)\n",
    "\n",
    "def onehot(data, nclass=10):\n",
    "    bsize = data.shape[0]\n",
    "    y = torch.zeros(bsize, nclass)\n",
    "    y.scatter_(1, data.unsqueeze(1), 1)\n",
    "    y = y.unsqueeze(2).unsqueeze(3)\n",
    "    return y\n",
    "\n",
    "# returns a batch of real images from the dataset (iterates infinitely on the dataset)\n",
    "def get_batch_real():\n",
    "    global iterator\n",
    "    try:\n",
    "        x_real, y_real = next(iterator)\n",
    "    except:\n",
    "        iterator = iter(dataloader)\n",
    "        x_real, y_real = next(iterator)\n",
    "    x_real = x_real.to(device)\n",
    "    y_real = onehot(y_real).to(device)\n",
    "    r_real = torch.full((x_real.size(0),), real_label, device=device)\n",
    "    return x_real, y_real, r_real\n",
    "\n",
    "# returns a batch of generated images and training targets y_fake\n",
    "# Note that the targets r_fake will be different is train_G is True or False\n",
    "def get_batch_fake(y_real, train_G=False):\n",
    "    z = torch.randn(y_real.shape[0], nz, 1, 1, device=device)\n",
    "    x_fake = netG(z, y_real).to(device)\n",
    "    if train_G:\n",
    "        r_fake = torch.ones((y_real.shape[0],)).to(device) # TODO\n",
    "    else:\n",
    "        r_fake = torch.zeros((y_real.shape[0],)).to(device) # TODO\n",
    "    return x_fake, y_real, r_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "**Reminder:** when your training loop starts to work, change the `workers` variable to 4 and rerun your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(steps):\n",
    "    \n",
    "    if i == steps_per_epoch * 11:\n",
    "        optimizerG.param_groups[0]['lr'] /= 10\n",
    "        optimizerD.param_groups[0]['lr'] /= 10\n",
    "        print(\"learning rate change!\")\n",
    "        \n",
    "    if i == steps_per_epoch * 16:\n",
    "        optimizerG.param_groups[0]['lr'] /= 10\n",
    "        optimizerD.param_groups[0]['lr'] /= 10\n",
    "        print(\"learning rate change!\")\n",
    "    \n",
    "    ########\n",
    "    # Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    for _ in range(nb_update_D):\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Create batches\n",
    "        x_real, y_real, r_real = get_batch_real()\n",
    "        x_fake, y_real, r_fake = get_batch_fake(y_real)\n",
    "        \n",
    "        # Forward \n",
    "        r_hat_real = netD(x_real,y_real).view((y_real.shape[0],)) # TODO\n",
    "        r_hat_fake = netD(x_fake,y_real).view((y_real.shape[0],)) # TODO\n",
    "        errD = criterion(r_hat_real,r_real) # TODO sum of criterion of real and fake samples\n",
    "        errD += criterion(r_hat_fake,r_fake)\n",
    "        \n",
    "        # Backward\n",
    "        # TODO backward & optimization step on D\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Compute / save metrics\n",
    "        avg_output_for_real = r_hat_real.mean().item()\n",
    "        avg_output_for_fake = r_hat_fake.mean().item()    \n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "\n",
    "\n",
    "    ########\n",
    "    # Update G network: maximize log(D(G(z)))\n",
    "    for _ in range(nb_update_G):\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # TODO: forward + backward\n",
    "        # NOTE: use errG as name for your loss variable, like errD above\n",
    "        # Create batches\n",
    "        x_real, y_real, r_real = get_batch_real()\n",
    "        x_fake, y_real, r_fake = get_batch_fake(y_real,train_G=True)\n",
    "        \n",
    "        # Forward \n",
    "        r_hat_fake = netD(x_fake,y_real).view((y_real.shape[0],)) \n",
    "        errG = criterion(r_hat_fake,r_fake) \n",
    "        \n",
    "        # Backward\n",
    "        # TODO backward & optimization step on D\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Compute / save metrics\n",
    "        G_losses.append(errG.item())\n",
    "        \n",
    "    ########\n",
    "    # Logs\n",
    "    if i % 25 == 0:\n",
    "        print('[%5d/%5d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f'\n",
    "              % (i, steps, errD.item(), errG.item(), avg_output_for_real, avg_output_for_fake))\n",
    "\n",
    "for i in range(steps):\n",
    "    \n",
    "    if i == steps_per_epoch * 11:\n",
    "        optimizerG.param_groups[0]['lr'] /= 10\n",
    "        optimizerD.param_groups[0]['lr'] /= 10\n",
    "        print(\"learning rate change!\")\n",
    "        \n",
    "    if i == steps_per_epoch * 16:\n",
    "        optimizerG.param_groups[0]['lr'] /= 10\n",
    "        optimizerD.param_groups[0]['lr'] /= 10\n",
    "        print(\"learning rate change!\")\n",
    "    \n",
    "    ########\n",
    "    # Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    for _ in range(nb_update_D):\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Create batches\n",
    "        x_real, y_real, r_real = get_batch_real()\n",
    "        x_fake, y_real, r_fake = get_batch_fake(y_real)\n",
    "        \n",
    "        # Forward \n",
    "        r_hat_real = netD(x_real,y_real).view((y_real.shape[0],)) # TODO\n",
    "        r_hat_fake = netD(x_fake,y_real).view((y_real.shape[0],)) # TODO\n",
    "        errD = criterion(r_hat_real,r_real) # TODO sum of criterion of real and fake samples\n",
    "        errD += criterion(r_hat_fake,r_fake)\n",
    "        \n",
    "        # Backward\n",
    "        # TODO backward & optimization step on D\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Compute / save metrics\n",
    "        avg_output_for_real = r_hat_real.mean().item()\n",
    "        avg_output_for_fake = r_hat_fake.mean().item()    \n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "\n",
    "\n",
    "    ########\n",
    "    # Update G network: maximize log(D(G(z)))\n",
    "    for _ in range(nb_update_G):\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # TODO: forward + backward\n",
    "        # NOTE: use errG as name for your loss variable, like errD above\n",
    "        # Create batches\n",
    "        x_real, y_real, r_real = get_batch_real()\n",
    "        x_fake, y_real, r_fake = get_batch_fake(y_real,train_G=True)\n",
    "        \n",
    "        # Forward \n",
    "        r_hat_fake = netD(x_fake,y_real).view((y_real.shape[0],)) \n",
    "        errG = criterion(r_hat_fake,r_fake) \n",
    "        \n",
    "        # Backward\n",
    "        # TODO backward & optimization step on D\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Compute / save metrics\n",
    "        G_losses.append(errG.item())\n",
    "        \n",
    "    ########\n",
    "    # Logs\n",
    "    if i % 100 == 0:\n",
    "        print('[%5d/%5d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f'\n",
    "              % (i, steps, errD.item(), errG.item(), avg_output_for_real, avg_output_for_fake))\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        with torch.no_grad():\n",
    "            x_fake = netG(fixed_noise, fixed_y).detach().cpu()\n",
    "            if arch == 'cGAN':\n",
    "                x_fake = x_fake.view((x_fake.shape[0],1,32,32))\n",
    "        img_list.append(vutils.make_grid(x_fake, padding=2, normalize=True, nrow=10))\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display training evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show generations\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss evolution\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"Generator Training Loss\")\n",
    "plt.plot(G_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"Generator Training Loss\")\n",
    "plt.plot(D_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
